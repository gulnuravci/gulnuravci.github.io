<!DOCTYPE html>
<html>
<title>Gulnur Avci | Projects | SpongeBob Character Identifier</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
<style>
    body {
        font-family: "Quicksand", Arial, sans-serif
    }
    .image-style {
        max-width: 100%;
        height: 450px;
        box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
        border-radius: 8px;
    }

    .mySlides {
        display:none;
        text-align: center;
        position: relative;
    }

    /* .mySlides img{
        height: 450px;
        width: auto;
    } */

    .caption {
        text-align: center;
        font-size: 17px; /* Adjust the font size as needed */
        color: black; /* Change the text color as needed */
    }
</style>

<body>
    <div class="w3-content" style="max-width:1500px; height: 650px">
        <header class="w3-container w3-xlarge w3-padding-24">
            <a href="#" class="w3-left w3-button w3-white" style="font-family:Playfair Display">Projects | Spongebob Character Identifier </a>
            <a href="../../index.html" class="w3-right w3-button w3-white" style="font-family:Playfair Display">Home</a>
        </header>

        <div class="w3-content w3-display-container">
            <div class="mySlides">
                <img class="image-style" src="../../images/Spongebob_Identifier/predictions.png">
                <p class="caption"><b>Figure 1:</b> Example test predictions made on one of the models.</p>
            </div>
            
            <div class="mySlides">
                <img class="image-style" src="../../images/Spongebob_Identifier/loss_graph_b0_b2.png">
                <p class="caption"><b>Figure 2:</b> Loss graph EfficientNet_B0 vs EfficientNet_B2.</p>
            </div>
            
            <div class="mySlides">
                <img class="image-style" src="../../images/Spongebob_Identifier/confusion_matrix.png">
                <p class="caption"><b>Figure 3:</b> Example confusion matrix made on one of the models.</p>
            </div>
            
            <div class="mySlides">
                <img class="image-style" src="../../images/Spongebob_Identifier/transformed_images.png">
                <p class="caption"><b>Figure 4:</b> Example transformations of input images to the model.</p>
            </div>
            
            <div class="mySlides">
                <img class="image-style" src="../../images/Spongebob_Identifier/loss_acc_curves_b2.png">
                <p class="caption"><b>Figure 5:</b> Loss and accuracy curve of the results of EfficientNet_B2 testing.</p>
            </div>

            <div class="mySlides">
                <img class="image-style" src="../../images/Spongebob_Identifier/pretrained_vit_loss.png">
                <p class="caption"><b>Figure 6:</b> Loss and accuracy curve of the results of pretrained ViT testing.</p>
            </div>

            <div class="mySlides">
                <img class="image-style" src="../../images/Spongebob_Identifier/vit_vs_effnetb2.png">
                <p class="caption"><b>Figure 7:</b> Experiment tracking parameters ViT vs EfficientNet_B2.</p>
            </div>

            <div class="mySlides">
                <img class="image-style" src="../../images/Spongebob_Identifier/inference_speed_vs_performance.png">
                <p class="caption"><b>Figure 8:</b> Inference Speed vs Performance for ViT vs EfficientNet_B2.</p>
            </div>

            <button class="w3-button w3-yellow w3-round-large w3-display-left" onclick="plusDivs(-1)">&#10094;</button>
            <button class="w3-button w3-yellow w3-round-large w3-display-right" onclick="plusDivs(1)">&#10095;</button>
        </div>

        <script>
            var slideIndex = 1;
            showSlides(slideIndex);

            function plusDivs(n) {
                showSlides(slideIndex += n);
            }

            function showSlides(n) {
                var slides = document.getElementsByClassName("mySlides");
                var captions = document.getElementsByClassName("caption");

                if (n > slides.length) {
                    slideIndex = 1;
                }
                if (n < 1) {
                    slideIndex = slides.length;
                }

                for (var i = 0; i < slides.length; i++) {
                    slides[i].style.display = "none";
                    captions[i].style.display = "none";
                }

                slides[slideIndex - 1].style.display = "block";
                captions[slideIndex - 1].style.display = "block";
            }
        </script>
    </div>

    <footer class="w3-light-grey w3-padding-64" id="about" style="margin-top: 20px; text-align: left;">
        <div style="max-width: 800px; margin: 0 auto;">
            <script
                type="module"
                src="https://gradio.s3-us-west-2.amazonaws.com/4.3.0/gradio.js"
            ></script>

            <gradio-app src="https://gulnuravci-spongebob-character-identifier.hf.space"></gradio-app>
            <a href="https://huggingface.co/spaces/gulnuravci/spongebob_character_identifier" style="text-decoration: none; color: #007BFF; font-weight: bold;">Link to Deployed Model on Hugging Face</a>
            <br>
            <a href="https://github.com/gulnuravci/spongebob_character_identifier" style="text-decoration: none; color: #007BFF; font-weight: bold;">Github link</a>
            <h2 style="font-family: 'Playfair Display'; color: #333; margin-top: 20px;">About</h2>
            <p>
                Completed a 24-hour <a href="https://www.youtube.com/watch?v=Z_ikDlimN6A&list=PLkGONtlaHAExMoTcyQqqFpZAMkODP4gLc&index=1&pp=gAQBiAQB" style="text-decoration: none; color: #007BFF;">Deep Learning with PyTorch</a> course by <b>David Bourke</b>, along with additional lessons on their <a href="https://www.learnpytorch.io" style="text-decoration: none; color: #007BFF;">website</a>. This course included the following topics:
            </p>
            <ul style="margin-left: 20px;">
                <li>PyTorch Fundamentals</li>
                <li>PyTorch Workflow Fundamentals</li>
                <li>Neural Network Classification</li>
                <li>Computer Vision</li>
                <li>Custom Datasets</li>
                <li>Going Modular</li>
                <li>Transfer Learning</li>
                <li>Experiment Tracking</li>
                <li>Paper Replicating</li>
                <li>Model Deployment</li>
            </ul>
            <p>With the skills I gained from this course, I developed my first PyTorch model. The model's purpose was to differentiate between various SpongeBob Squarepants characters, starting with a dataset containing 10 different characters.</p>
            <p>I began by experimenting with different versions of the existing <a href="https://poloclub.github.io/cnn-explainer/">Tiny VGG</a>, which is a lightweight convolutional neural network architecture. During these experiments, I made changes to parameters like the number of CNN layers, hidden layers, training epochs, learning rates, and dataset size.</p>
            <p>With these initial methods, I achieved an accuracy of approximately 70%. The likely reason for this is that I had to manually collect my training data. This process involved watching SpongeBob episodes to observe the evolving styles of character drawings across various seasons. I took screenshots from different angles and situations in the episodes. Given the time-consuming nature of this task, I've managed to gather only 120 images per character thus far.</p>
            <p>I needed either a much larger dataset, or needed to use transfer learning. Transfer learning is a suitable approach for this problem because the initial training on a vast dataset using a much more complex architecture equips the model with a general understanding of what different images look like. This understanding remains valuable, even when your dataset is relatively unrelated to the training data they initially used. By adding an additional classifier layer on top of the pre-trained model, you can quickly fine-tune the model to differentiate between specific classes, even with a smaller dataset, as was the case for me. Using transfer learning, my model was able to achieve an accuracy of over 98%.</p>
            <p>In the process of creating my first model, I conducted experiments using PyTorch's pre-trained classification models 'EfficientNet_B0' and 'EfficientNet_B2' as well. Through these experiments, I observed that 'EfficientNet_B2' exhibited higher efficiency. This observation is supported by the loss curve, as indicated in the figure above, where the 'EfficientNet_B2' model achieved lower loss within the same training period. Notably, 'EfficientNet_B2' features a larger parameter count and was trained on a more extensive dataset.</p>
            <p>Next, I delved into the topic of paper replication, which involves translating machine learning research papers into practical code. The paper I analyzed was titled <a href="https://arxiv.org/pdf/2010.11929.pdf">"An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale"</a>. Transformers are a type of deep learning model architecture initially developed for natural language processing. They employ self-attention mechanisms to process and understand sequences of data, such as sentences. In simpler terms, transformers discern which words are more significant than others in order to grasp the meaning of the data.</p>
            <p>The paper I mentioned above held significant importance upon its release, as it pioneered the use of Vision Transformers (ViT) to apply a deep learning technique, originally designed for natural language processing, to the domain of image processing. This is primarily achieved by dividing images into patches of a defined size and arranging them in a manner where each patch is treated like a word. The model can then identify relationships between these image patches.</p>
            <p>There existed four main equations within the paper that allowed me to completely replicate the model: </p>
            <ul style="margin-left: 20px;">
                <li><b>Equation 1: Split data into patches and creating the class, position and patch embedding</b> - This involves breaking the image into patches and representing it in a 1D format.</li>
                <li><b>Equation 2: Multi-Head Attention (MSA)</b> - part of the Transformer Encoder, applies layer normalization, utilizes simplified approach where all queries (q), keys (k), and values (v) are set to the same value for efficient processing.</li>
                <li><b>Equation 3: Multilayer Perceptron (MLP)</b> - part of the Transformer Encoder, applies layer normalization, the remaining process essentially consists of multiple combinations of linear and non-linear layers.</li>
                <li><b>Equation 4: Classifier</b> - converts the resulting logits into probability predictions for each class.</li>
            </ul>
            <p>While recreating the ViT architecture was highly educational, its performance was suboptimal compared to the original ViT model from the research paper, which was trained on a vast dataset, whereas mine was trained on just over a thousand pictures. For this reason, I found it beneficial to use PyTorch's pretrained ViT model implementation, which resulted in significantly improved accuracy.</p>
            <p>After experimenting with various models and techniques, I conducted experiment tracking to determine the most suitable model for my project. I imported my code into Google Colab to leverage GPU acceleration for efficient training and testing of both EfficientNet_B2 and ViT models, allowing for a detailed comparison.</p>
            <p>Figure 7 provides an overview of the experiment tracking parameters comparing ViT and EfficientNet_B2 models. While test loss and test accuracy show a close correlation, ViT's number of parameters and model size is approximately 11 times that of EfficientNet_B2. Additionally, the time per prediction is nearly five times longer for ViT. Given my intention to deploy the model on a mobile device for real-time predictions, it is essential to maintain low inference times. Consequently, EfficientNet_B2 emerges as the preferred model choice for my application, boasting an accuracy of 96.5% and an inference time of 0.14 seconds. It's worth noting that these predictions were made using Google Colab's CPU, which tends to be slower than other device CPUs, suggesting that inference times on other devices would likely be even faster. It's important to clarify that this choice doesn't imply that EfficientNet_B2 is inherently better than ViT; it simply suits our current use case. Different experiment results may arise under different conditions, such as larger datasets and longer training times, among other factors.</p>
        </div>

    </footer>

</body>
</html>

